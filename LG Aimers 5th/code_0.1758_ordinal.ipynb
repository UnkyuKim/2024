{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "895d5190",
   "metadata": {
    "id": "895d5190"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3f20371",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d3f20371",
    "outputId": "ee55a8c7-5727-4150-816b-ea89c125b2d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 데이터:  (40506, 464)\n",
      "test 데이터:  (17361, 465)\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = \"data\"\n",
    "RANDOM_STATE = 110\n",
    "\n",
    "# Load data\n",
    "train_data = pd.read_csv(ROOT_DIR+ \"/train.csv\")\n",
    "test_data = pd.read_csv(ROOT_DIR+ \"/test.csv\")\n",
    "\n",
    "print(\"train 데이터: \",train_data.shape)\n",
    "print(\"test 데이터: \",test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6pSiXDilBKn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6pSiXDilBKn",
    "outputId": "1befdf64-2f8a-4745-f2c2-79cea777b94a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전부 null인 칼럼 제거 후  (40506, 186)\n"
     ]
    }
   ],
   "source": [
    "#전부 null인 칼럼 제거\n",
    "cols = train_data.columns\n",
    "null_list = []\n",
    "\n",
    "for i in cols:\n",
    "    if train_data[i].isna().sum() == train_data.shape[0]:\n",
    "        null_list.append(i)\n",
    "\n",
    "train_not_null = train_data.drop(columns=null_list)\n",
    "print(\"전부 null인 칼럼 제거 후 \",train_not_null.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "CnKUNypl6LtC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CnKUNypl6LtC",
    "outputId": "0d9182c9-bf92-4f64-c26e-6b58297cde57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대회 정정사항 null인 칼럼 제거 후  (40506, 178)\n"
     ]
    }
   ],
   "source": [
    "# 8개의 NULL(OK) 포함 칼럼 제거\n",
    "columns_with_ok = [col for col in train_not_null.columns if train_not_null[col].eq('OK').any()]\n",
    "tmp = train_not_null[columns_with_ok]\n",
    "additional_null = tmp.columns[tmp.isna().sum() > 0].tolist()\n",
    "train_not_null.drop(columns = additional_null,inplace=True)\n",
    "print(\"대회 정정사항 null인 칼럼 제거 후 \",train_not_null.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "JIGCqM8SnBMX",
   "metadata": {
    "id": "JIGCqM8SnBMX"
   },
   "outputs": [],
   "source": [
    "COORDINATE_column_nnull = [col for col in train_not_null.columns if 'coordinate' in col.lower()]\n",
    "POSITION_columns_nnull = [col for col in train_not_null.columns if 'position' in col.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Kw_6pewfmfQe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kw_6pewfmfQe",
    "outputId": "901f68cc-cc9e-4808-94b2-e337745faddd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "print(len(COORDINATE_column_nnull))\n",
    "print(len(POSITION_columns_nnull))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "Yb_xP7Ih6m-F",
   "metadata": {
    "id": "Yb_xP7Ih6m-F"
   },
   "outputs": [],
   "source": [
    "# 거리 계산 함수 정의\n",
    "def calculate_distances(df, threshold):\n",
    "    # 거리 계산을 위한 빈 데이터프레임 생성\n",
    "    distances = pd.DataFrame(index=df.index)\n",
    "    distances['value'] = df['value']  # 기존 값 복사\n",
    "    distances['distance'] = np.nan  # 거리 칼럼 초기화\n",
    "\n",
    "    # 빈도수가 높은 값들을 기준으로 처리\n",
    "    while not df.empty:\n",
    "        # 가장 빈도수가 높은 값과 그 빈도수 선택\n",
    "        most_frequent_value = df['value'].iloc[0]\n",
    "\n",
    "        # 해당 기준값으로부터 threshold 범위 이내의 값을 처리\n",
    "        df_within_threshold = df[abs(df['value'] - most_frequent_value) <= threshold]\n",
    "        df_outside_threshold = df[abs(df['value'] - most_frequent_value) > threshold]\n",
    "\n",
    "        # 기준 값과의 거리 계산\n",
    "        distances.loc[df_within_threshold.index, 'distance'] = df_within_threshold['value'].apply(lambda x: abs(x - most_frequent_value))\n",
    "\n",
    "        # 최빈값은 거리가 0이 되도록\n",
    "        distances.loc[df_within_threshold.index[df_within_threshold['value'] == most_frequent_value], 'distance'] = 0\n",
    "        # 범위 이내의 값 처리 후 제거\n",
    "        df = df_outside_threshold\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "y9UyGpFI6nfY",
   "metadata": {
    "id": "y9UyGpFI6nfY"
   },
   "outputs": [],
   "source": [
    "# train_coordinate =train_not_null[COORDINATE_column_nnull]\n",
    "# # 기준 거리 설정\n",
    "# threshold = 40\n",
    "\n",
    "# # 원본 데이터프레임의 모든 칼럼에 대해 반복\n",
    "# for col in train_coordinate.columns:\n",
    "#     print(f\"Processing column: {col}\")\n",
    "\n",
    "#     # value_counts로부터 데이터프레임 생성\n",
    "#     value_counts = train_coordinate[col].value_counts()\n",
    "#     df = value_counts.reset_index()\n",
    "#     df.columns = ['value', 'count']\n",
    "\n",
    "#     # 거리 계산\n",
    "#     distances = calculate_distances(df, threshold)\n",
    "\n",
    "#     # 원본 데이터에 거리 값을 적용\n",
    "#     def apply_distances(value):\n",
    "#         if value in distances['value'].values:\n",
    "#             return distances.loc[distances['value'] == value, 'distance'].values[0]\n",
    "#         else:\n",
    "#             return np.nan\n",
    "\n",
    "#    # 새 데이터프레임 생성\n",
    "#     train_coordinate_copy = train_coordinate.copy()\n",
    "#     # 거리 값을 적용할 새로운 칼럼 생성\n",
    "#     train_coordinate_copy[col + '_distance'] = train_coordinate_copy[col].apply(apply_distances)\n",
    "\n",
    "#     # 원본 데이터프레임에 새로운 칼럼 추가\n",
    "#     # .loc 사용하여 칼럼을 추가합니다.\n",
    "#     train_coordinate.loc[:, col + '_distance'] = train_coordinate_copy[col + '_distance']\n",
    "\n",
    "# # 결과 출력\n",
    "# print(train_coordinate.head())\n",
    "\n",
    "# 매번 돌리기 시간이 오래 걸리는 코드로 미리 데이터 파일로 만들어 두겠음\n",
    "train_position = pd.read_csv(\"train_coordinate.csv\")\n",
    "\n",
    "# for 문을 통해 원래 데이터에 덮어씌우기\n",
    "for col in COORDINATE_column_nnull:\n",
    "    train_not_null[col] = train_position[f\"{col}_distance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dKMFXMt6nhb",
   "metadata": {
    "id": "6dKMFXMt6nhb"
   },
   "outputs": [],
   "source": [
    "# train_position=train_not_null[POSITION_columns_nnull]\n",
    "# # 기준 거리 설정\n",
    "# threshold = 20\n",
    "# # 원본 데이터프레임의 모든 칼럼에 대해 반복\n",
    "# for col in train_position.columns:\n",
    "#     print(f\"Processing column: {col}\")\n",
    "\n",
    "#     # value_counts로부터 데이터프레임 생성\n",
    "#     value_counts = train_position[col].value_counts()\n",
    "#     df = value_counts.reset_index()\n",
    "#     df.columns = ['value', 'count']\n",
    "\n",
    "#     # 거리 계산\n",
    "#     distances = calculate_distances(df, threshold)\n",
    "\n",
    "#     # 원본 데이터에 거리 값을 적용\n",
    "#     def apply_distances(value):\n",
    "#         if value in distances['value'].values:\n",
    "#             return distances.loc[distances['value'] == value, 'distance'].values[0]\n",
    "#         else:\n",
    "#             return np.nan\n",
    "\n",
    "#    # 새 데이터프레임 생성\n",
    "#     train_position_copy = train_position.copy()\n",
    "#     # 거리 값을 적용할 새로운 칼럼 생성\n",
    "#     train_position_copy[col + '_distance'] = train_position_copy[col].apply(apply_distances)\n",
    "\n",
    "#     # 원본 데이터프레임에 새로운 칼럼 추가\n",
    "#     # .loc 사용하여 칼럼을 추가합니다.\n",
    "#     train_position.loc[:, col + '_distance'] = train_position_copy[col + '_distance']\n",
    "\n",
    "# # 결과 출력\n",
    "# print(train_position.head())\n",
    "\n",
    "# 매번 돌리기 시간이 오래 걸리는 코드로 미리 데이터 파일로 만들어 두겠음\n",
    "train_position = pd.read_csv(\"train_position.csv\")\n",
    "# for 문을 통해 원래 데이터에 덮어씌우기\n",
    "for col in POSITION_columns_nnull:\n",
    "    train_not_null[col] = train_position[f\"{col}_distance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "I0r9cnQl6nkw",
   "metadata": {
    "id": "I0r9cnQl6nkw"
   },
   "outputs": [],
   "source": [
    "# test 데이터에도 같은 과정을 진행해야한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f0df8ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f0df8ca",
    "outputId": "299c6290-bd74-479f-e42e-43f824b1bb64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "하나의 값만 가지는 칼럼 제거 후  (40506, 114)\n"
     ]
    }
   ],
   "source": [
    "#하나의 값만 가지는 칼럼 제거\n",
    "cols2 = train_not_null.columns\n",
    "unique_cols = []\n",
    "\n",
    "for i in cols2:\n",
    "    if train_not_null[i].nunique() == 1:\n",
    "        unique_cols.append(i)\n",
    "\n",
    "train_final = train_not_null.drop(columns = unique_cols )\n",
    "print(\"하나의 값만 가지는 칼럼 제거 후 \",train_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e22226e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3e22226e",
    "outputId": "3604c0a9-78ae-4088-e1c0-0f9c55333619"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코딩 중복 제거 후 :  (40506, 90)\n"
     ]
    }
   ],
   "source": [
    "# 인코딩 후 중복을 칼럼을 제거합니다.\n",
    "train_drop_dup= train_final.copy()\n",
    "\n",
    "for col in train_drop_dup.columns:\n",
    "    order = train_drop_dup[col].value_counts().index.tolist()\n",
    "    order_map = {value: idx for idx, value in enumerate(order)}\n",
    "    tmp = pd.Categorical(train_drop_dup[col], categories=order, ordered=True)\n",
    "    train_drop_dup[col] = tmp.codes\n",
    "\n",
    "list_dup = train_drop_dup.columns[train_drop_dup.T.duplicated(keep=False)]\n",
    "duplicate_rows = train_drop_dup.T\n",
    "\n",
    "train_drop_dup = train_drop_dup.T.drop_duplicates().T\n",
    "cols_drop_dup = train_drop_dup.columns\n",
    "\n",
    "train_final = train_final[cols_drop_dup]\n",
    "print(\"인코딩 중복 제거 후 : \",train_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "FDqPxQ6YBiOb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDqPxQ6YBiOb",
    "outputId": "c08ce1d4-32d0-4011-b137-d9567f82c53d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null 칼럼 제거 후 :  (40506, 90)\n"
     ]
    }
   ],
   "source": [
    "# null 제거인데 이미 위에서 진행햇음\n",
    "a= train_final.isnull().sum() > 0\n",
    "a= a.index\n",
    "null_list = a[train_final.isnull().sum() > 0]\n",
    "\n",
    "train_final = train_final.drop(columns=null_list)\n",
    "print(\"null 칼럼 제거 후 : \",train_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "UBxGfi9pLNVs",
   "metadata": {
    "id": "UBxGfi9pLNVs"
   },
   "outputs": [],
   "source": [
    "COORDINATE_columns = [col for col in train_final.columns if 'coordinate' in col.lower()]\n",
    "POSITION_columns = [col for col in train_final.columns if 'position' in col.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6A6hUYWQcHVQ",
   "metadata": {
    "id": "6A6hUYWQcHVQ"
   },
   "outputs": [],
   "source": [
    "# 지금까지 칼럼을 전처리 하는 방법은 끝이고 이제 이어서 각 데이터 칼럼 값을 바꾸는 과정을 진행할 예정으로 train,test 모두 처리해주어야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "zRyO1xufaETw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zRyO1xufaETw",
    "outputId": "48068121-b081-4ed0-811c-4090af06f156"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17361, 90)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "test_not_null = test_data[train_final.columns]\n",
    "test_not_null.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "VPgSTekKCMvv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VPgSTekKCMvv",
    "outputId": "1c913e77-bd61-4a9c-b927-aac7a556f714"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_420/3504433409.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_not_null[col] = test_coordinate[f\"{col}_distance\"]\n"
     ]
    }
   ],
   "source": [
    "# 우선 앞서 진행한 coordinate , position 전처리를 해준다.\n",
    "# (사실 train , test concat 한다음에 진행했어야하는데 시각화랑 이것 저것 이미 진행해서 test데이터 에서만 동일하게 진행하겠음)\n",
    "\n",
    "# test_coordinate =test_data[COORDINATE_column_nnull]\n",
    "# # 기준 거리 설정\n",
    "# threshold = 40\n",
    "\n",
    "# # 원본 데이터프레임의 모든 칼럼에 대해 반복\n",
    "# for col in test_coordinate.columns:\n",
    "#     print(f\"Processing column: {col}\")\n",
    "\n",
    "#     # value_counts로부터 데이터프레임 생성\n",
    "#     value_counts = test_coordinate[col].value_counts()\n",
    "#     df = value_counts.reset_index()\n",
    "#     df.columns = ['value', 'count']\n",
    "\n",
    "#     # 거리 계산\n",
    "#     distances = calculate_distances(df, threshold)\n",
    "\n",
    "#     # 원본 데이터에 거리 값을 적용\n",
    "#     def apply_distances(value):\n",
    "#         if value in distances['value'].values:\n",
    "#             return distances.loc[distances['value'] == value, 'distance'].values[0]\n",
    "#         else:\n",
    "#             return np.nan\n",
    "\n",
    "#    # 새 데이터프레임 생성\n",
    "#     test_coordinate_copy = test_coordinate.copy()\n",
    "#     # 거리 값을 적용할 새로운 칼럼 생성\n",
    "#     test_coordinate_copy[col + '_distance'] = test_coordinate_copy[col].apply(apply_distances)\n",
    "\n",
    "#     # 원본 데이터프레임에 새로운 칼럼 추가\n",
    "#     # .loc 사용하여 칼럼을 추가합니다.\n",
    "#     test_coordinate.loc[:, col + '_distance'] = test_coordinate_copy[col + '_distance']\n",
    "\n",
    "# # 결과 출력\n",
    "# print(test_coordinate.head())\n",
    "\n",
    "# 매번 돌리기 시간이 오래 걸리는 코드로 미리 데이터 파일로 만들어 두겠음\n",
    "test_coordinate = pd.read_csv(\"test_coordinate.csv\")\n",
    "for col in COORDINATE_columns:\n",
    "    test_not_null[col] = test_coordinate[f\"{col}_distance\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "C7vc7wjJDTzk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7vc7wjJDTzk",
    "outputId": "d3c5d3f9-9ea4-4861-c6b3-86e07fff6617"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_420/619731773.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_not_null[col] = test_coordinate[f\"{col}_distance\"]\n"
     ]
    }
   ],
   "source": [
    "test_position=test_data[POSITION_columns_nnull]\n",
    "# 기준 거리 설정\n",
    "threshold = 20\n",
    "# 원본 데이터프레임의 모든 칼럼에 대해 반복\n",
    "\n",
    "# for col in test_position.columns:\n",
    "#     print(f\"Processing column: {col}\")\n",
    "\n",
    "#     # value_counts로부터 데이터프레임 생성\n",
    "#     value_counts = test_position[col].value_counts()\n",
    "#     df = value_counts.reset_index()\n",
    "#     df.columns = ['value', 'count']\n",
    "\n",
    "#     # 거리 계산\n",
    "#     distances = calculate_distances(df, threshold)\n",
    "\n",
    "#     # 원본 데이터에 거리 값을 적용\n",
    "#     def apply_distances(value):\n",
    "#         if value in distances['value'].values:\n",
    "#             return distances.loc[distances['value'] == value, 'distance'].values[0]\n",
    "#         else:\n",
    "#             return np.nan\n",
    "\n",
    "#    # 새 데이터프레임 생성\n",
    "#     test_position_copy = test_position.copy()\n",
    "#     # 거리 값을 적용할 새로운 칼럼 생성\n",
    "#     test_position_copy[col + '_distance'] = test_position_copy[col].apply(apply_distances)\n",
    "\n",
    "#     # 원본 데이터프레임에 새로운 칼럼 추가\n",
    "#     # .loc 사용하여 칼럼을 추가합니다.\n",
    "#     test_position.loc[:, col + '_distance'] = test_position_copy[col + '_distance']\n",
    "\n",
    "# # 결과 출력\n",
    "# print(test_position.head())\n",
    "\n",
    "# 매번 돌리기 시간이 오래 걸리는 코드로 미리 데이터 파일로 만들어 두겠음\n",
    "test_coordinate = pd.read_csv(\"test_position.csv\")\n",
    "for col in POSITION_columns:\n",
    "    test_not_null[col] = test_coordinate[f\"{col}_distance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dTI4OYupMSyJ",
   "metadata": {
    "id": "dTI4OYupMSyJ"
   },
   "outputs": [],
   "source": [
    "test_data = test_not_null.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfe23d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.read_csv(ROOT_DIR+ \"/important_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "IpgfKGWAzpJH",
   "metadata": {
    "id": "IpgfKGWAzpJH"
   },
   "outputs": [],
   "source": [
    "train_bins = train_final.copy()\n",
    "test_bins = test_data.copy()\n",
    "new_bins = new_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "gy6JNZm4sN4Q",
   "metadata": {
    "id": "gy6JNZm4sN4Q"
   },
   "outputs": [],
   "source": [
    "Time_columns = [col for col in train_final.columns if 'Time' in col]\n",
    "\n",
    "train_bins[Time_columns[0]] = train_final[Time_columns[0]] / train_final[Time_columns[3]]\n",
    "train_bins[Time_columns[1]] = train_final[Time_columns[1]] / train_final[Time_columns[3]]\n",
    "train_bins[Time_columns[2]] = train_final[Time_columns[2]] / train_final[Time_columns[3]]\n",
    "\n",
    "test_bins[Time_columns[0]] = test_data[Time_columns[0]] / test_data[Time_columns[3]]\n",
    "test_bins[Time_columns[1]] = test_data[Time_columns[1]] / test_data[Time_columns[3]]\n",
    "test_bins[Time_columns[2]] = test_data[Time_columns[2]] / test_data[Time_columns[3]]\n",
    "\n",
    "new_bins[Time_columns[0]] = new_data[Time_columns[0]] / new_data[Time_columns[3]]\n",
    "new_bins[Time_columns[1]] = new_data[Time_columns[1]] / new_data[Time_columns[3]]\n",
    "new_bins[Time_columns[2]] = new_data[Time_columns[2]] / new_data[Time_columns[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "PpClaXD3BIKs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "PpClaXD3BIKs",
    "outputId": "307fc1ad-f185-4441-a004-e806099d203e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam       29\n",
       "DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1     24\n",
       "DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1     16\n",
       "DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1     16\n",
       "DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam       20\n",
       "DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam       19\n",
       "Machine Tact time Collect Result_Fill2                    42\n",
       "Machine Tact time Collect Result_Fill1                   194\n",
       "Machine Tact time Collect Result_Dam                     266\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_columns = list(set([col for col in train_final.columns if 'time' in col.lower()]) - set(Time_columns))\n",
    "train_final[time_columns].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "AU677yrUO7NJ",
   "metadata": {
    "id": "AU677yrUO7NJ"
   },
   "outputs": [],
   "source": [
    "#time_columns 구간화 하는것이 성능이 더 잘나왔음 구간화 이후 타겟 인코딩 순서가 있으므로 Ordinal Encoding은 어떨까?\n",
    "for col in time_columns:\n",
    "    # 훈련 데이터와 테스트 데이터를 합쳐서 두 데이터셋 모두의 범위를 포함하도록 설정\n",
    "    combined_data = pd.concat([train_final[col], test_data[col]])\n",
    "\n",
    "    # 합쳐진 데이터로부터 Sturges' 규칙을 사용하여 구간 경계값 계산\n",
    "    bins = np.histogram_bin_edges(combined_data, bins='sturges')\n",
    "\n",
    "    # 구간 중심값 계산\n",
    "    bin_centers = [(bin_edge_left + bin_edge_right) / 2 for bin_edge_left, bin_edge_right in zip(bins[:-1], bins[1:])]\n",
    "\n",
    "    # 훈련 데이터의 구간화 (구간을 왼쪽, 오른쪽 경계 모두 포함하여 설정)\n",
    "    binned_train = pd.cut(train_final[col], bins=bins, include_lowest=True)\n",
    "    bin_mapping = dict(zip(binned_train.cat.categories, bin_centers))\n",
    "    train_bins[col] = binned_train.map(bin_mapping)\n",
    "\n",
    "    # 테스트 데이터의 구간화\n",
    "    binned_test = pd.cut(test_data[col], bins=bins, include_lowest=True)\n",
    "    test_bins[col] = binned_test.map(bin_mapping)\n",
    "    \n",
    "    #new_data\n",
    "    binned_new = pd.cut(new_data[col], bins=bins, include_lowest=True)\n",
    "    bin_mapping = dict(zip(binned_new.cat.categories, bin_centers))\n",
    "    new_bins[col] = binned_new.map(bin_mapping)\n",
    "\n",
    "    # 결측치 처리: 구간 밖의 값들 처리\n",
    "    # 각 구간 중심값 계산\n",
    "    first_bin_center = bin_centers[0]\n",
    "    last_bin_center = bin_centers[-1]\n",
    "\n",
    "    # 훈련 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "    train_bins.loc[train_final[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "    train_bins.loc[train_final[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n",
    "\n",
    "    # 테스트 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "    test_bins.loc[test_data[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "    test_bins.loc[test_data[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n",
    "\n",
    "    # new 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "    new_bins.loc[new_data[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "    new_bins.loc[new_data[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "uNIHhlj0YLzL",
   "metadata": {
    "id": "uNIHhlj0YLzL"
   },
   "outputs": [],
   "source": [
    "# 'Machine Tact time'이 포함된 열 이름 리스트 생성\n",
    "Machine_Tact_time_columns = [col for col in train_final.columns if 'Machine Tact time' in col]\n",
    "\n",
    "for col in Machine_Tact_time_columns:\n",
    "    # 훈련 데이터와 테스트 데이터를 합쳐서 두 데이터셋 모두의 범위를 포함하도록 설정\n",
    "    combined_data = pd.concat([train_final[col], test_data[col]])\n",
    "\n",
    "    # 합쳐진 데이터로부터 Sturges' 규칙을 사용하여 구간 경계값 계산\n",
    "    bins = np.histogram_bin_edges(combined_data, bins='sturges')\n",
    "\n",
    "    # 구간 중심값 계산\n",
    "    bin_centers = [(bin_edge_left + bin_edge_right) / 2 for bin_edge_left, bin_edge_right in zip(bins[:-1], bins[1:])]\n",
    "\n",
    "    # 훈련 데이터의 구간화 (구간을 왼쪽, 오른쪽 경계 모두 포함하여 설정)\n",
    "    binned_train = pd.cut(train_final[col], bins=bins, include_lowest=True)\n",
    "    bin_mapping = dict(zip(binned_train.cat.categories, bin_centers))\n",
    "    train_bins[col] = binned_train.map(bin_mapping)\n",
    "\n",
    "    # 테스트 데이터의 구간화\n",
    "    binned_test = pd.cut(test_data[col], bins=bins, include_lowest=True)\n",
    "    test_bins[col] = binned_test.map(bin_mapping)\n",
    "\n",
    "\n",
    "    binned_new = pd.cut(new_data[col], bins=bins, include_lowest=True)\n",
    "    bin_mapping = dict(zip(binned_new.cat.categories, bin_centers))\n",
    "    new_bins[col] = binned_new.map(bin_mapping)\n",
    "    \n",
    "    # 결측치 처리: 구간 밖의 값들 처리\n",
    "    # 각 구간 중심값 계산\n",
    "    first_bin_center = bin_centers[0]\n",
    "    last_bin_center = bin_centers[-1]\n",
    "\n",
    "    # 훈련 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "    train_bins.loc[train_final[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "    train_bins.loc[train_final[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n",
    "\n",
    "    # 테스트 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "    test_bins.loc[test_data[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "    test_bins.loc[test_data[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n",
    "\n",
    "        # new 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "    new_bins.loc[new_data[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "    new_bins.loc[new_data[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "tBgaeuTqsufh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBgaeuTqsufh",
    "outputId": "73d10f99-0c00-4bde-8a78-0aec2d202520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pallet_columns: 3\n",
      "Qty_columns: 3\n",
      "Receip_columns: 3\n"
     ]
    }
   ],
   "source": [
    "#Pallet 칼럼들 전처리 (3개의 칼럼)\n",
    "Pallet_columns = [col for col in train_final.columns if 'pallet' in col.lower()]\n",
    "\n",
    "#Qty 칼럼들 전처리 (3개의 칼럼)\n",
    "#Qty 칼럼들 전부 구간화 하는 방법은 폐기\n",
    "Qty_columns = [col for col in train_final.columns if 'qty' in col.lower()]\n",
    "\n",
    "#Receip 칼럼들 전처리 (3개의 칼럼)\n",
    "#Receip 칼럼들 전부 구간화 하는 방법은 폐기\n",
    "Receip_columns = [col for col in train_final.columns if 'receip' in col.lower()]\n",
    "train_final[Receip_columns]\n",
    "\n",
    "print(\"Pallet_columns:\" ,len(Pallet_columns))\n",
    "print(\"Qty_columns:\" ,len(Qty_columns))\n",
    "print(\"Receip_columns:\" ,len(Receip_columns))\n",
    "# 총 9개의 칼럼은 중요도가 높아 잘 학습할 수 있도록 처리해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "PI-pJGm8Tnb8",
   "metadata": {
    "id": "PI-pJGm8Tnb8"
   },
   "outputs": [],
   "source": [
    "#값들을 확인해보니 거의다 같지만 몇몇 행들만 값이 달랐음 다름을 학습시키려함\n",
    "\n",
    "train_bins[Qty_columns[2]] = train_bins[Qty_columns[1]] == train_bins[Qty_columns[2]]\n",
    "train_bins[Qty_columns[1]] = train_bins[Qty_columns[0]] == train_bins[Qty_columns[1]]\n",
    "\n",
    "train_bins[Receip_columns[2]] = train_bins[Receip_columns[1]] == train_bins[Receip_columns[2]]\n",
    "train_bins[Receip_columns[1]] = train_bins[Receip_columns[0]] == train_bins[Receip_columns[1]]\n",
    "\n",
    "test_bins[Qty_columns[2]] = test_bins[Qty_columns[1]] == test_bins[Qty_columns[2]]\n",
    "test_bins[Qty_columns[1]] = test_bins[Qty_columns[0]] == test_bins[Qty_columns[1]]\n",
    "\n",
    "test_bins[Receip_columns[2]] = test_bins[Receip_columns[1]] == test_bins[Receip_columns[2]]\n",
    "test_bins[Receip_columns[1]] = test_bins[Receip_columns[0]] == test_bins[Receip_columns[1]]\n",
    "\n",
    "new_bins[Qty_columns[2]] = new_bins[Qty_columns[1]] == new_bins[Qty_columns[2]]\n",
    "new_bins[Qty_columns[1]] = new_bins[Qty_columns[0]] == new_bins[Qty_columns[1]]\n",
    "\n",
    "new_bins[Receip_columns[2]] = new_bins[Receip_columns[1]] == new_bins[Receip_columns[2]]\n",
    "new_bins[Receip_columns[1]] = new_bins[Receip_columns[0]] == new_bins[Receip_columns[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "vciKyMgeX7eJ",
   "metadata": {
    "id": "vciKyMgeX7eJ"
   },
   "outputs": [],
   "source": [
    "# #Qty_columns 구간화\n",
    "# col = Qty_columns[0]\n",
    "# combined_data = pd.concat([train_final[col], test_data[col]])\n",
    "\n",
    "# bins = np.histogram_bin_edges(combined_data, bins='sturges')\n",
    "\n",
    "# bin_centers = [(bin_edge_left + bin_edge_right) / 2 for bin_edge_left, bin_edge_right in zip(bins[:-1], bins[1:])]\n",
    "\n",
    "# binned_train = pd.cut(train_final[col], bins=bins, include_lowest=True)\n",
    "# bin_mapping = dict(zip(binned_train.cat.categories, bin_centers))\n",
    "# train_bins[col] = binned_train.map(bin_mapping)\n",
    "\n",
    "# binned_test = pd.cut(test_data[col], bins=bins, include_lowest=True)\n",
    "# test_bins[col] = binned_test.map(bin_mapping)\n",
    "\n",
    "# # 각 구간 중심값 계산\n",
    "# first_bin_center = bin_centers[0]\n",
    "# last_bin_center = bin_centers[-1]\n",
    "\n",
    "# # 훈련 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "# train_bins.loc[train_final[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "# train_bins.loc[train_final[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n",
    "\n",
    "# # 테스트 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "# test_bins.loc[test_data[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "# test_bins.loc[test_data[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "nz-2TUWJWUo6",
   "metadata": {
    "id": "nz-2TUWJWUo6"
   },
   "outputs": [],
   "source": [
    "# #Receip_columns 구간화\n",
    "# col=Receip_columns[0]\n",
    "# combined_data = pd.concat([train_final[col], test_data[col]])\n",
    "\n",
    "# bins = np.histogram_bin_edges(combined_data, bins='sturges')\n",
    "\n",
    "# bin_centers = [(bin_edge_left + bin_edge_right) / 2 for bin_edge_left, bin_edge_right in zip(bins[:-1], bins[1:])]\n",
    "\n",
    "# binned_train = pd.cut(train_final[col], bins=bins, include_lowest=True)\n",
    "# bin_mapping = dict(zip(binned_train.cat.categories, bin_centers))\n",
    "# train_bins[col] = binned_train.map(bin_mapping)\n",
    "\n",
    "# binned_test = pd.cut(test_data[col], bins=bins, include_lowest=True)\n",
    "# test_bins[col] = binned_test.map(bin_mapping)\n",
    "\n",
    "# # 각 구간 중심값 계산\n",
    "# first_bin_center = bin_centers[0]\n",
    "# last_bin_center = bin_centers[-1]\n",
    "\n",
    "# # 훈련 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "# train_bins.loc[train_final[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "# train_bins.loc[train_final[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n",
    "\n",
    "# # 테스트 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "# test_bins.loc[test_data[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "# test_bins.loc[test_data[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3yg7xSKui_Jn",
   "metadata": {
    "id": "3yg7xSKui_Jn"
   },
   "outputs": [],
   "source": [
    "# #Pallet_ID ID의 의미라면 구간화는 진행하지 않고 카테고리로 변환해야할듯\n",
    "for col in Pallet_columns:\n",
    "    train_bins[col] = train_bins[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "tYWZxSsxgfDv",
   "metadata": {
    "id": "tYWZxSsxgfDv"
   },
   "outputs": [],
   "source": [
    "COORDINATE_columns = [col for col in train_final.columns if 'coordinate' in col.lower()]\n",
    "POSITION_columns = [col for col in train_final.columns if 'position' in col.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "irGbtFTY-cNP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "irGbtFTY-cNP",
    "outputId": "0b73f625-2d05-4ce7-e5c1-2b9fc7852780"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "지금까지의 칼럼을 제외한 칼럼 44\n"
     ]
    }
   ],
   "source": [
    "integration_columns = time_columns+Pallet_columns+Qty_columns+Receip_columns+COORDINATE_columns+POSITION_columns\n",
    "remain_columns= list(set(train_final.columns.tolist()) - set(integration_columns))\n",
    "\n",
    "print(\"지금까지의 칼럼을 제외한 칼럼\",len(remain_columns))\n",
    "\n",
    "train_remain =train_final[remain_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kKRSkDJYGsLo",
   "metadata": {
    "id": "kKRSkDJYGsLo"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "rLLf7_bwIFMC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "rLLf7_bwIFMC",
    "outputId": "3d2d75f5-c8d4-4644-eac3-ce4a53ae9995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispense_Volume_columns의 개수 4\n",
      "Distance_Speed_columns 개수 15\n",
      "DISCHARGED_SPEED_columns 개수 2\n",
      "THICKNESS_columns 개수 3\n",
      "Pressure_columns 개수 6\n",
      "WorkMode_columns 개수 3\n",
      "CURE_SPEED_columns 개수 2\n",
      "list_binary 개수 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model.Suffix_Dam                            7\n",
       "Workorder_Dam                             663\n",
       "Chamber Temp. Collect Result_AutoClave     26\n",
       "Chamber Temp. Unit Time_AutoClave          24\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dispense_Volume_columns = [col for col in train_remain.columns if 'Dispense Volume' in col]\n",
    "print(\"Dispense_Volume_columns의 개수\" ,len(Dispense_Volume_columns))\n",
    "\n",
    "Distance_Speed_columns = [col for col in train_remain.columns if 'Distance Speed' in col]\n",
    "print(\"Distance_Speed_columns 개수\" ,len(Distance_Speed_columns))\n",
    "\n",
    "Distance_Speed_Line_columns =[col for col in Distance_Speed_columns if 'Line' in col]\n",
    "Distance_Speed_Circle_columns =[col for col in Distance_Speed_columns if 'Circle' in col]\n",
    "\n",
    "DISCHARGED_SPEED_columns = [col for col in train_remain.columns if 'DISCHARGED SPEED' in col]\n",
    "print(\"DISCHARGED_SPEED_columns 개수\" ,len(DISCHARGED_SPEED_columns))\n",
    "\n",
    "THICKNESS_columns = [col for col in train_remain.columns if 'THICKNESS' in col]\n",
    "print(\"THICKNESS_columns 개수\" ,len(THICKNESS_columns))\n",
    "\n",
    "Pressure_columns= [col for col in train_remain.columns if 'Pressure' in col]\n",
    "print(\"Pressure_columns 개수\" ,len(Pressure_columns))\n",
    "\n",
    "WorkMode_columns= [col for col in train_remain.columns if 'WorkMode' in col]\n",
    "print(\"WorkMode_columns 개수\" ,len(WorkMode_columns))\n",
    "\n",
    "CURE_SPEED_columns= [col for col in train_remain.columns if 'CURE SPEED' in col]\n",
    "print(\"CURE_SPEED_columns 개수\" ,len(CURE_SPEED_columns))\n",
    "\n",
    "list_binary = train_remain.columns[train_remain.nunique() ==2].tolist()\n",
    "print(\"list_binary 개수\" ,len(list_binary))\n",
    "\n",
    "q= Dispense_Volume_columns + Distance_Speed_columns + DISCHARGED_SPEED_columns+THICKNESS_columns+Pressure_columns +WorkMode_columns+CURE_SPEED_columns +list_binary\n",
    "\n",
    "train_remain2 = train_remain[list(set(train_remain.columns) - set(q))]\n",
    "train_remain2.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wX2LStJdyUxA",
   "metadata": {
    "id": "wX2LStJdyUxA"
   },
   "source": [
    "*   Dispense_Volume_columns:quantile 4개로 구간화\n",
    "*   Distance_Speed_columns:quantile 5개로 구간화\n",
    "*   DISCHARGED_SPEED_columns:sturges구간화\n",
    "*   THICKNESS_columns:sturges구간화\n",
    "*   Pressure_columns:sturges구간화\n",
    "*   WorkMode_columns:카테고리\n",
    "*   CURE_SPEED_columns:sturges구간화\n",
    "*   list_binary:이진화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5qX4aEk1P9A",
   "metadata": {
    "id": "d5qX4aEk1P9A"
   },
   "outputs": [],
   "source": [
    "for col in Dispense_Volume_columns:\n",
    "    # 분위수 구간화\n",
    "    quantile_bins = pd.qcut(train_final[col], q=4, duplicates='drop') #4분위 구간화\n",
    "\n",
    "    # 각 구간의 최빈값 계산\n",
    "    bin_modes = {}\n",
    "    bin_edges = quantile_bins.cat.categories\n",
    "    for bin_label in bin_edges:\n",
    "        bin_data = train_final[col][quantile_bins == bin_label]\n",
    "        bin_modes[bin_label] = bin_data.mode().values[0]  # 최빈값\n",
    "\n",
    "    # 훈련 데이터의 구간화 및 최빈값 매핑\n",
    "    binned_train = pd.cut(train_final[col], bins=bin_edges, include_lowest=True)\n",
    "    train_bins[col] = binned_train.map(bin_modes)\n",
    "\n",
    "    # 테스트 데이터의 구간화 및 최빈값 매핑\n",
    "    binned_test = pd.cut(test_data[col], bins=bin_edges, include_lowest=True)\n",
    "    test_bins[col] = binned_test.map(bin_modes)\n",
    "\n",
    "    \n",
    "    binned_new = pd.cut(new_data[col], bins=bin_edges, include_lowest=True)\n",
    "    new_bins[col] = binned_new.map(bin_modes)\n",
    "\n",
    "    # 결측치 처리: 구간 밖의 값들 처리\n",
    "    first_bin_mode = bin_modes[list(bin_modes.keys())[0]]\n",
    "    last_bin_mode = bin_modes[list(bin_modes.keys())[-1]]\n",
    "\n",
    "    # 훈련 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "    train_bins.loc[train_final[col] < bin_edges[0].left, col] = first_bin_mode  # 처음 구간보다 작은 값\n",
    "    train_bins.loc[train_final[col] > bin_edges[-1].right, col] = last_bin_mode  # 마지막 구간보다 큰 값\n",
    "\n",
    "    # 테스트 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "    test_bins.loc[test_data[col] < bin_edges[0].left, col] = first_bin_mode  # 처음 구간보다 작은 값\n",
    "    test_bins.loc[test_data[col] > bin_edges[-1].right, col] = last_bin_mode  # 마지막 구간보다 큰 값\n",
    "    \n",
    "    new_bins.loc[new_data[col] < bin_edges[0].left, col] = first_bin_mode  # 처음 구간보다 작은 값\n",
    "    new_bins.loc[new_data[col] > bin_edges[-1].right, col] = last_bin_mode  # 마지막 구간보다 큰 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "OPV-UgiA2iIj",
   "metadata": {
    "id": "OPV-UgiA2iIj"
   },
   "outputs": [],
   "source": [
    "for col in Distance_Speed_columns:\n",
    "    # 분위수 구간화\n",
    "    quantile_bins = pd.qcut(train_final[col], q=4, duplicates='drop') #5분위 구간화\n",
    "\n",
    "    # 각 구간의 최빈값 계산\n",
    "    bin_modes = {}\n",
    "    bin_edges = quantile_bins.cat.categories\n",
    "    for bin_label in bin_edges:\n",
    "        bin_data = train_final[col][quantile_bins == bin_label]\n",
    "        bin_modes[bin_label] = bin_data.mode().values[0]  # 최빈값\n",
    "\n",
    "    # 훈련 데이터의 구간화 및 최빈값 매핑\n",
    "    binned_train = pd.cut(train_final[col], bins=bin_edges, include_lowest=True)\n",
    "    train_bins[col] = binned_train.map(bin_modes)\n",
    "\n",
    "    # 테스트 데이터의 구간화 및 최빈값 매핑\n",
    "    binned_test = pd.cut(test_data[col], bins=bin_edges, include_lowest=True)\n",
    "    test_bins[col] = binned_test.map(bin_modes)\n",
    "\n",
    "    \n",
    "    binned_new = pd.cut(new_data[col], bins=bin_edges, include_lowest=True)\n",
    "    new_bins[col] = binned_new.map(bin_modes)\n",
    "    # 결측치 처리: 구간 밖의 값들 처리\n",
    "    first_bin_mode = bin_modes[list(bin_modes.keys())[0]]\n",
    "    last_bin_mode = bin_modes[list(bin_modes.keys())[-1]]\n",
    "\n",
    "    # 훈련 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "    train_bins.loc[train_final[col] < bin_edges[0].left, col] = first_bin_mode  # 처음 구간보다 작은 값\n",
    "    train_bins.loc[train_final[col] > bin_edges[-1].right, col] = last_bin_mode  # 마지막 구간보다 큰 값\n",
    "\n",
    "    # 테스트 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "    test_bins.loc[test_data[col] < bin_edges[0].left, col] = first_bin_mode  # 처음 구간보다 작은 값\n",
    "    test_bins.loc[test_data[col] > bin_edges[-1].right, col] = last_bin_mode  # 마지막 구간보다 큰 값\n",
    "    \n",
    "    new_bins.loc[new_data[col] < bin_edges[0].left, col] = first_bin_mode  # 처음 구간보다 작은 값\n",
    "    new_bins.loc[new_data[col] > bin_edges[-1].right, col] = last_bin_mode  # 마지막 구간보다 큰 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d-AHi0C94AF_",
   "metadata": {
    "id": "d-AHi0C94AF_"
   },
   "outputs": [],
   "source": [
    "for col in DISCHARGED_SPEED_columns:\n",
    "    # 훈련 데이터와 테스트 데이터를 합쳐서 두 데이터셋 모두의 범위를 포함하도록 설정\n",
    "    combined_data = pd.concat([train_final[col], test_data[col]])\n",
    "\n",
    "    # 합쳐진 데이터로부터 Sturges' 규칙을 사용하여 구간 경계값 계산\n",
    "    bins = np.histogram_bin_edges(combined_data, bins='sturges')\n",
    "\n",
    "    # 구간 중심값 계산\n",
    "    bin_centers = [(bin_edge_left + bin_edge_right) / 2 for bin_edge_left, bin_edge_right in zip(bins[:-1], bins[1:])]\n",
    "\n",
    "    # 훈련 데이터의 구간화 (구간을 왼쪽, 오른쪽 경계 모두 포함하여 설정)\n",
    "    binned_train = pd.cut(train_final[col], bins=bins, include_lowest=True)\n",
    "    bin_mapping = dict(zip(binned_train.cat.categories, bin_centers))\n",
    "    train_bins[col] = binned_train.map(bin_mapping)\n",
    "\n",
    "    # 테스트 데이터의 구간화\n",
    "    binned_test = pd.cut(test_data[col], bins=bins, include_lowest=True)\n",
    "    test_bins[col] = binned_test.map(bin_mapping)\n",
    "    \n",
    "    binned_new = pd.cut(new_data[col], bins=bins, include_lowest=True)\n",
    "    new_bins[col] = binned_new.map(bin_mapping)\n",
    "\n",
    "    # 결측치 처리: 구간 밖의 값들 처리\n",
    "    # 각 구간 중심값 계산\n",
    "    first_bin_center = bin_centers[0]\n",
    "    last_bin_center = bin_centers[-1]\n",
    "\n",
    "    # 훈련 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "    train_bins.loc[train_final[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "    train_bins.loc[train_final[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n",
    "\n",
    "    # 테스트 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "    test_bins.loc[test_data[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "    test_bins.loc[test_data[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n",
    "    \n",
    "    new_bins.loc[new_data[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "    new_bins.loc[new_data[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cKuL9nr84AX6",
   "metadata": {
    "id": "cKuL9nr84AX6"
   },
   "outputs": [],
   "source": [
    "for col in THICKNESS_columns:\n",
    "    # 훈련 데이터와 테스트 데이터를 합쳐서 두 데이터셋 모두의 범위를 포함하도록 설정\n",
    "    combined_data = pd.concat([train_final[col], test_data[col]])\n",
    "\n",
    "    # 합쳐진 데이터로부터 Sturges' 규칙을 사용하여 구간 경계값 계산\n",
    "    bins = np.histogram_bin_edges(combined_data, bins='sturges')\n",
    "\n",
    "    # 구간 중심값 계산\n",
    "    bin_centers = [(bin_edge_left + bin_edge_right) / 2 for bin_edge_left, bin_edge_right in zip(bins[:-1], bins[1:])]\n",
    "\n",
    "    # 훈련 데이터의 구간화 (구간을 왼쪽, 오른쪽 경계 모두 포함하여 설정)\n",
    "    binned_train = pd.cut(train_final[col], bins=bins, include_lowest=True)\n",
    "    bin_mapping = dict(zip(binned_train.cat.categories, bin_centers))\n",
    "    train_bins[col] = binned_train.map(bin_mapping)\n",
    "\n",
    "    # 테스트 데이터의 구간화\n",
    "    binned_test = pd.cut(test_data[col], bins=bins, include_lowest=True)\n",
    "    test_bins[col] = binned_test.map(bin_mapping)\n",
    "\n",
    "    \n",
    "    binned_new = pd.cut(new_data[col], bins=bins, include_lowest=True)\n",
    "    new_bins[col] = binned_new.map(bin_mapping)\n",
    "    \n",
    "    # 결측치 처리: 구간 밖의 값들 처리\n",
    "    # 각 구간 중심값 계산\n",
    "    first_bin_center = bin_centers[0]\n",
    "    last_bin_center = bin_centers[-1]\n",
    "\n",
    "    # 훈련 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "    train_bins.loc[train_final[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "    train_bins.loc[train_final[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n",
    "\n",
    "    # 테스트 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "    test_bins.loc[test_data[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "    test_bins.loc[test_data[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n",
    "\n",
    "    \n",
    "    new_bins.loc[new_data[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "    new_bins.loc[new_data[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "H1toXfMy4AvK",
   "metadata": {
    "id": "H1toXfMy4AvK"
   },
   "outputs": [],
   "source": [
    "for col in Pressure_columns:\n",
    "    # 훈련 데이터와 테스트 데이터를 합쳐서 두 데이터셋 모두의 범위를 포함하도록 설정\n",
    "    combined_data = pd.concat([train_final[col], test_data[col]])\n",
    "\n",
    "    # 합쳐진 데이터로부터 Sturges' 규칙을 사용하여 구간 경계값 계산\n",
    "    bins = np.histogram_bin_edges(combined_data, bins='sturges')\n",
    "\n",
    "    # 구간 중심값 계산\n",
    "    bin_centers = [(bin_edge_left + bin_edge_right) / 2 for bin_edge_left, bin_edge_right in zip(bins[:-1], bins[1:])]\n",
    "\n",
    "    # 훈련 데이터의 구간화 (구간을 왼쪽, 오른쪽 경계 모두 포함하여 설정)\n",
    "    binned_train = pd.cut(train_final[col], bins=bins, include_lowest=True)\n",
    "    bin_mapping = dict(zip(binned_train.cat.categories, bin_centers))\n",
    "    train_bins[col] = binned_train.map(bin_mapping)\n",
    "\n",
    "    # 테스트 데이터의 구간화\n",
    "    binned_test = pd.cut(test_data[col], bins=bins, include_lowest=True)\n",
    "    test_bins[col] = binned_test.map(bin_mapping)\n",
    "\n",
    "    \n",
    "    binned_new = pd.cut(new_data[col], bins=bins, include_lowest=True)\n",
    "    new_bins[col] = binned_new.map(bin_mapping)\n",
    "    \n",
    "    # 결측치 처리: 구간 밖의 값들 처리\n",
    "    # 각 구간 중심값 계산\n",
    "    first_bin_center = bin_centers[0]\n",
    "    last_bin_center = bin_centers[-1]\n",
    "\n",
    "    # 훈련 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "    train_bins.loc[train_final[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "    train_bins.loc[train_final[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n",
    "\n",
    "    # 테스트 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "    test_bins.loc[test_data[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "    test_bins.loc[test_data[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n",
    "\n",
    "    new_bins.loc[new_data[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "    new_bins.loc[new_data[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fTsC0ZV_4GXb",
   "metadata": {
    "id": "fTsC0ZV_4GXb"
   },
   "outputs": [],
   "source": [
    "#WorkMode_columns MODE의 의미라면 구간화는 진행하지 않고 카테고리로 변환해야할듯\n",
    "for col in WorkMode_columns:\n",
    "    train_bins[col] = train_bins[col].astype('category')\n",
    "    \n",
    "for col in WorkMode_columns:\n",
    "    new_bins[col] = new_bins[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "InPc9LXv4FVq",
   "metadata": {
    "id": "InPc9LXv4FVq"
   },
   "outputs": [],
   "source": [
    "for col in CURE_SPEED_columns:\n",
    "    # 훈련 데이터와 테스트 데이터를 합쳐서 두 데이터셋 모두의 범위를 포함하도록 설정\n",
    "    combined_data = pd.concat([train_final[col], test_data[col]])\n",
    "\n",
    "    # 합쳐진 데이터로부터 Sturges' 규칙을 사용하여 구간 경계값 계산\n",
    "    bins = np.histogram_bin_edges(combined_data, bins='sturges')\n",
    "\n",
    "    # 구간 중심값 계산\n",
    "    bin_centers = [(bin_edge_left + bin_edge_right) / 2 for bin_edge_left, bin_edge_right in zip(bins[:-1], bins[1:])]\n",
    "\n",
    "    # 훈련 데이터의 구간화 (구간을 왼쪽, 오른쪽 경계 모두 포함하여 설정)\n",
    "    binned_train = pd.cut(train_final[col], bins=bins, include_lowest=True)\n",
    "    bin_mapping = dict(zip(binned_train.cat.categories, bin_centers))\n",
    "    train_bins[col] = binned_train.map(bin_mapping)\n",
    "\n",
    "    # 테스트 데이터의 구간화\n",
    "    binned_test = pd.cut(test_data[col], bins=bins, include_lowest=True)\n",
    "    test_bins[col] = binned_test.map(bin_mapping)\n",
    "    \n",
    "    binned_new = pd.cut(new_data[col], bins=bins, include_lowest=True)\n",
    "    new_bins[col] = binned_new.map(bin_mapping)\n",
    "\n",
    "    # 결측치 처리: 구간 밖의 값들 처리\n",
    "    # 각 구간 중심값 계산\n",
    "    first_bin_center = bin_centers[0]\n",
    "    last_bin_center = bin_centers[-1]\n",
    "\n",
    "    # 훈련 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "    train_bins.loc[train_final[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "    train_bins.loc[train_final[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n",
    "\n",
    "    # 테스트 데이터에서 구간 경계를 벗어나는 값 처리\n",
    "    test_bins.loc[test_data[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "    test_bins.loc[test_data[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n",
    "\n",
    "    new_bins.loc[new_data[col] < bins[0], col] = first_bin_center  # 처음 구간보다 작은 값\n",
    "    new_bins.loc[new_data[col] > bins[-1], col] = last_bin_center  # 마지막 구간보다 큰 값\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0GsbW_-JSdsc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0GsbW_-JSdsc",
    "outputId": "ce36b9fb-dac7-4932-afcd-7bcd34c47519"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "칼럼의 길이의 총 합:  40\n",
      "train_remain 의 칼럼개수 - train_remain2의 칼럼개수  40\n",
      "겹치는 칼럼이 없음을 확인\n"
     ]
    }
   ],
   "source": [
    "# 이 순서로 처리하면 겹치는 칼럼이 없음을 확인\n",
    "\n",
    "print(\"칼럼의 길이의 총 합: \",len(Dispense_Volume_columns) +len(Distance_Speed_columns) +len(DISCHARGED_SPEED_columns) +len(THICKNESS_columns) +len(Pressure_columns) +len(WorkMode_columns) +len(CURE_SPEED_columns) +len(list_binary))\n",
    "print(\"train_remain 의 칼럼개수 - train_remain2의 칼럼개수 \",train_remain.shape[1] - train_remain2.shape[1])\n",
    "\n",
    "if (len(Dispense_Volume_columns) +len(Distance_Speed_columns) +len(DISCHARGED_SPEED_columns) +len(THICKNESS_columns) +len(Pressure_columns) +len(WorkMode_columns) +len(CURE_SPEED_columns) +len(list_binary)) == train_remain.shape[1] - train_remain2.shape[1]:\n",
    "    print(\"겹치는 칼럼이 없음을 확인\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "TrrUOnbVbF6F",
   "metadata": {
    "id": "TrrUOnbVbF6F"
   },
   "outputs": [],
   "source": [
    "# 값이 두개인 경우 먼저 True/ False 로 처리\n",
    "list_binary.remove('target')\n",
    "for col in list_binary:\n",
    "    most_frequent_value = train_bins[col].mode()[0]  # 가장 빈도가 높은 값\n",
    "\n",
    "    train_bins[col] = train_bins[col] == most_frequent_value\n",
    "    train_bins[col] = train_bins[col].astype(bool)\n",
    "\n",
    "    test_bins[col] = test_bins[col] == most_frequent_value\n",
    "    test_bins[col] = test_bins[col].astype(bool)\n",
    "    \n",
    "    new_bins[col] = new_bins[col] == most_frequent_value\n",
    "    new_bins[col] = new_bins[col].astype(bool)\n",
    "\n",
    "test_bins.drop('target',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6nROJyhMvycG",
   "metadata": {
    "id": "6nROJyhMvycG"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "F4y8zBm7wAKo",
   "metadata": {
    "id": "F4y8zBm7wAKo"
   },
   "outputs": [],
   "source": [
    "train_bins['target'] = train_final['target']\n",
    "train_pre = train_bins\n",
    "test_pre = test_bins\n",
    "new_pre = new_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "IvRxWPChNR_q",
   "metadata": {
    "id": "IvRxWPChNR_q"
   },
   "outputs": [],
   "source": [
    "COORDINATE_columns = [col for col in train_final.columns if 'coordinate' in col.lower()]\n",
    "POSITION_columns = [col for col in train_final.columns if 'position' in col.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "BR7PhtbSveaN",
   "metadata": {
    "id": "BR7PhtbSveaN"
   },
   "outputs": [],
   "source": [
    "#이거는 추가로 구간화 한 칼럼들 ORDINAL 인코딩 코드\n",
    "train_pre['target'] = train_pre['target'].map({'Normal': 1, 'AbNormal': 0})\n",
    "\n",
    "Ordinal_Encoding_columns =Dispense_Volume_columns + Distance_Speed_columns + DISCHARGED_SPEED_columns+THICKNESS_columns+Pressure_columns+CURE_SPEED_columns # 구간화한 칼럼들\n",
    "cols_target_encoding = train_pre.select_dtypes(include=['category', 'object','bool']).columns.tolist()\n",
    "cols_target_encoding = list(set(cols_target_encoding) - set(Ordinal_Encoding_columns))\n",
    "cols_standard_encoding = train_pre.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "cols_standard_encoding.remove('target')\n",
    "\n",
    "\n",
    "new_pre['target'] = new_pre['target'].map({'Normal': 1, 'AbNormal': 0})\n",
    "\n",
    "Ordinal_Encoding_columns =Dispense_Volume_columns + Distance_Speed_columns + DISCHARGED_SPEED_columns+THICKNESS_columns+Pressure_columns+CURE_SPEED_columns # 구간화한 칼럼들\n",
    "cols_target_encoding = new_pre.select_dtypes(include=['category', 'object','bool']).columns.tolist()\n",
    "cols_target_encoding = list(set(cols_target_encoding) - set(Ordinal_Encoding_columns))\n",
    "cols_standard_encoding = new_pre.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "cols_standard_encoding.remove('target')\n",
    "\n",
    "\n",
    "if (len(train_pre.columns) -1 == len(cols_standard_encoding) + len(cols_target_encoding)):\n",
    "  print(\"모든 칼럼이 인코딩 예정입니다\")\n",
    "\n",
    "target_enc = ce.TargetEncoder(cols=cols_target_encoding)\n",
    "train_data_encoded = target_enc.fit_transform(train_pre[cols_target_encoding], train_pre['target'])\n",
    "test_data_encoded = target_enc.transform(test_pre[cols_target_encoding])\n",
    "\n",
    "new_data_encoded = target_enc.fit_transform(new_pre[cols_target_encoding], new_pre['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e2bd7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#이거는 추가로 구간화 한 칼럼들 타겟인코딩한 코드\n",
    "train_pre['target'] = train_pre['target'].map({'Normal': 1, 'AbNormal': 0})\n",
    "\n",
    "cols_target_encoding = train_pre.select_dtypes(include=['category', 'object','bool']).columns.tolist()\n",
    "cols_standard_encoding = train_pre.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "cols_standard_encoding.remove('target')\n",
    "\n",
    "new_pre['target'] = new_pre['target'].map({'Normal': 1, 'AbNormal': 0})\n",
    "\n",
    "cols_target_encoding = new_pre.select_dtypes(include=['category', 'object','bool']).columns.tolist()\n",
    "cols_standard_encoding = new_pre.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "cols_standard_encoding.remove('target')\n",
    "\n",
    "if (len(train_pre.columns) -1 == len(cols_standard_encoding) + len(cols_target_encoding)):\n",
    "  print(\"모든 칼럼이 인코딩 예정입니다\")\n",
    "\n",
    "target_enc = ce.TargetEncoder(cols=cols_target_encoding)\n",
    "train_data_encoded = target_enc.fit_transform(train_pre[cols_target_encoding], train_pre['target'])\n",
    "test_data_encoded = target_enc.transform(test_pre[cols_target_encoding])\n",
    "\n",
    "new_data_encoded = target_enc.fit_transform(new_pre[cols_target_encoding], new_pre['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2c1caf1d",
   "metadata": {
    "id": "Qg55hyhLvYof"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['HEAD Standby Position Y Collect Result_Dam'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols_standard_encoding:\n\u001b[0;32m----> 6\u001b[0m     train_data_encoded[col] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mtrain_pre\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      7\u001b[0m     test_data_encoded[col] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(test_pre[[col]])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:6176\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6174\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6175\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['HEAD Standby Position Y Collect Result_Dam'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model = StandardScaler()\n",
    "\n",
    "for col in cols_standard_encoding:\n",
    "    train_data_encoded[col] = model.fit_transform(train_pre[[col]])\n",
    "    test_data_encoded[col] = model.transform(test_pre[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75673c56",
   "metadata": {
    "id": "cSwL2u7JAUZk"
   },
   "outputs": [],
   "source": [
    "# 추가로 구간화 한 칼럼 타겟 인코딩 했으면 진행 x\n",
    "for col in Ordinal_Encoding_columns:\n",
    "    # 고유한 범주를 크기순으로 정렬\n",
    "    unique_values = train_pre[col].unique()\n",
    "    sorted_values = sorted(unique_values, reverse=True)  # 크기순으로 정렬\n",
    "\n",
    "    # 범주를 정수로 매핑\n",
    "    ordinal_mapping = {value: i for i, value in enumerate(sorted_values)}\n",
    "\n",
    "    # 데이터프레임에 Ordinal Encoding 적용\n",
    "    train_data_encoded[col] = train_pre[col].map(ordinal_mapping)\n",
    "\n",
    "    if train_data_encoded[col].max()!=0:\n",
    "        train_data_encoded[col] = train_data_encoded[col] / train_data_encoded[col].max()\n",
    "    else:\n",
    "        train_data_encoded[col] =0\n",
    "\n",
    "    test_data_encoded[col] = test_pre[col].map(ordinal_mapping)\n",
    "\n",
    "    if test_data_encoded[col].max()!=0:\n",
    "        test_data_encoded[col] = test_data_encoded[col] / test_data_encoded[col].max()\n",
    "    else:\n",
    "        test_data_encoded[col] =0\n",
    "        \n",
    "    new_data_encoded[col] = new_pre[col].map(ordinal_mapping)\n",
    "\n",
    "    if new_data_encoded[col].max()!=0:\n",
    "        new_data_encoded[col] = new_data_encoded[col] / new_data_encoded[col].max()\n",
    "    else:\n",
    "        new_data_encoded[col] =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "o6HVeFsgvdCR",
   "metadata": {
    "id": "o6HVeFsgvdCR"
   },
   "outputs": [],
   "source": [
    "train_data_encoded['target'] = train_final['target']\n",
    "train_data_encoded['target'] = train_data_encoded['target'].map({'Normal': 1, 'AbNormal': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "DDxHjkCkvkoq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "id": "DDxHjkCkvkoq",
    "outputId": "70130de7-861c-4238-a0be-e6715c766c40",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total: Normal: 43014, AbNormal: 2899\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    2899\n",
       "1    2899\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_ratio = 1.0  # 1.0 means 1:1 ratio\n",
    "\n",
    "new_data['target'] = new_data['target'].map({'Normal': 1, 'AbNormal': 0})\n",
    "df_normal_new = new_data[new_data[\"target\"] == 1] # Normal\n",
    "df_abnormal_new = new_data[new_data[\"target\"] == 0] # AbNormal\n",
    "\n",
    "df_normal = train_data_encoded[train_data_encoded[\"target\"] == 1] # Normal\n",
    "df_abnormal = train_data_encoded[train_data_encoded[\"target\"] == 0] # AbNormal\n",
    "df_normal_concat = pd.concat([df_normal, df_normal_new], axis=0).reset_index(drop=True)\n",
    "df_abnormal_concat = pd.concat([df_abnormal, df_abnormal_new], axis=0).reset_index(drop=True)\n",
    "\n",
    "num_normal_concat = len(df_normal_concat)\n",
    "num_abnormal_concat = len(df_abnormal_concat)\n",
    "print(f\"  Total: Normal: {num_normal_concat}, AbNormal: {num_abnormal_concat}\")\n",
    "\n",
    "\n",
    "\n",
    "df_normal_concat = df_normal.sample(n=int(num_abnormal_concat * normal_ratio), replace=False, random_state=RANDOM_STATE)\n",
    "df_concat = pd.concat([df_normal_concat, df_abnormal_concat], axis=0).reset_index(drop=True)\n",
    "df_concat.value_counts(\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf30c27",
   "metadata": {},
   "source": [
    "# df_concat['target'] = df_concat['target'].astype('category')\n",
    "\n",
    "columns_to_visualize = df_concat.columns.drop('target')\n",
    "\n",
    "for column in columns_to_visualize:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # Boxplot으로 시각화 (target은 범주형, column은 숫자형이어야 함)\n",
    "    sns.boxplot(x='target', y=column, data=df_concat)\n",
    "    \n",
    "    # 그래프 제목 설정\n",
    "    plt.title(f'{column} boxplot by target')\n",
    "    \n",
    "    # 그래프 표시\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # Boxplot으로 시각화\n",
    "    sns.violinplot(x='target', y=column, data=df_concat, inner=\"quartile\")\n",
    "    \n",
    "    # 그래프 제목 설정\n",
    "    plt.title(f'{column} violinplot by target')\n",
    "    \n",
    "    # 그래프 표시\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "DpnqVeA9vlBd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DpnqVeA9vlBd",
    "outputId": "c7408eee-7973-460c-a188-c67387a6d1a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \tAbnormal\tNormal\n",
      "  Total: Normal: 2174, AbNormal: 2174 ratio: 1.0\n",
      "  Total: Normal: 725, AbNormal: 725 ratio: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분할\n",
    "df_train, df_val = train_test_split(\n",
    "    df_concat,\n",
    "    test_size=0.25, # 0.25 일때가 최고 성능 보임 (0.2, 0.24, 0.26, 0.27, 0.3, 0.35)\n",
    "    stratify=df_concat[\"target\"],\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "\n",
    "def print_stats(df: pd.DataFrame):\n",
    "    num_normal = len(df[df[\"target\"] == 1])\n",
    "    num_abnormal = len(df[df[\"target\"] == 0])\n",
    "\n",
    "    print(f\"  Total: Normal: {num_normal}, AbNormal: {num_abnormal}\" + f\" ratio: {num_abnormal/num_normal}\")\n",
    "\n",
    "\n",
    "# Print statistics\n",
    "print(f\"  \\tAbnormal\\tNormal\")\n",
    "print_stats(df_train)\n",
    "print_stats(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b35dfc6c",
   "metadata": {
    "id": "b35dfc6c"
   },
   "outputs": [],
   "source": [
    "# 1. Feature와 Target 분리\n",
    "X_train = df_train.drop(columns=['target'])\n",
    "y_train = df_train['target']\n",
    "\n",
    "X_val = df_val.drop(columns=['target'])\n",
    "y_val = df_val['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773b1fb8",
   "metadata": {
    "id": "773b1fb8"
   },
   "source": [
    "### 3. 피쳐 중요도 시각화 및 중요도에 따른 가중치 부여"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a18ce8",
   "metadata": {
    "id": "06a18ce8"
   },
   "source": [
    "### 3.1 SHAP Value 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "SujDf8WmtD3O",
   "metadata": {
    "id": "SujDf8WmtD3O"
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "RANDOM_STATE = 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9516717b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "9516717b",
    "outputId": "3dfb9b4d-3bce-44ca-be92-1b9e00fc5560"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Equipment_Dam: object, Model.Suffix_Dam: object, Workorder_Dam: object, Chamber Temp. Judge Value_AutoClave: object, Equipment_Fill1: object, Equipment_Fill2: object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 모델 학습 (예: XGBoost)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBClassifier(random_state\u001b[38;5;241m=\u001b[39mRANDOM_STATE,enable_categorical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/sklearn.py:1512\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_\n\u001b[1;32m   1511\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m-> 1512\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m \u001b[43m_wrap_evaluation_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1515\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1518\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin_eval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_qid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_dmatrix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m   1532\u001b[0m     params,\n\u001b[1;32m   1533\u001b[0m     train_dmatrix,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1542\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[1;32m   1543\u001b[0m )\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/sklearn.py:596\u001b[0m, in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical, feature_types)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_evaluation_matrices\u001b[39m(\n\u001b[1;32m    577\u001b[0m     missing: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    578\u001b[0m     X: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m     feature_types: Optional[FeatureTypes],\n\u001b[1;32m    593\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Any, List[Tuple[Any, \u001b[38;5;28mstr\u001b[39m]]]:\n\u001b[1;32m    594\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert array_like evaluation matrices into DMatrix.  Perform validation on the\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;124;03m    way.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 596\u001b[0m     train_dmatrix \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dmatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    610\u001b[0m     n_validation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m eval_set \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(eval_set)\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_or_none\u001b[39m(meta: Optional[Sequence], name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Sequence:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/sklearn.py:1003\u001b[0m, in \u001b[0;36mXGBModel._create_dmatrix\u001b[0;34m(self, ref, **kwargs)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _can_use_qdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree_method) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbooster \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1003\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mQuantileDMatrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnthread\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_bin\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `QuantileDMatrix` supports lesser types than DMatrix\u001b[39;00m\n\u001b[1;32m   1007\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/core.py:1573\u001b[0m, in \u001b[0;36mQuantileDMatrix.__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, max_bin, ref, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001b[0m\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   1554\u001b[0m         info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1555\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1566\u001b[0m         )\n\u001b[1;32m   1567\u001b[0m     ):\n\u001b[1;32m   1568\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1569\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf data iterator is used as input, data like label should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1570\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecified as batch argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1571\u001b[0m         )\n\u001b[0;32m-> 1573\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1580\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1581\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_lower_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_upper_bound\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/core.py:1632\u001b[0m, in \u001b[0;36mQuantileDMatrix._init\u001b[0;34m(self, data, ref, enable_categorical, **meta)\u001b[0m\n\u001b[1;32m   1620\u001b[0m config \u001b[38;5;241m=\u001b[39m make_jcargs(\n\u001b[1;32m   1621\u001b[0m     nthread\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnthread, missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing, max_bin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_bin\n\u001b[1;32m   1622\u001b[0m )\n\u001b[1;32m   1623\u001b[0m ret \u001b[38;5;241m=\u001b[39m _LIB\u001b[38;5;241m.\u001b[39mXGQuantileDMatrixCreateFromCallback(\n\u001b[1;32m   1624\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1625\u001b[0m     it\u001b[38;5;241m.\u001b[39mproxy\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1630\u001b[0m     ctypes\u001b[38;5;241m.\u001b[39mbyref(handle),\n\u001b[1;32m   1631\u001b[0m )\n\u001b[0;32m-> 1632\u001b[0m \u001b[43mit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;66;03m# delay check_call to throw intermediate exception first\u001b[39;00m\n\u001b[1;32m   1634\u001b[0m _check_call(ret)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/core.py:569\u001b[0m, in \u001b[0;36mDataIter.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    567\u001b[0m exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/core.py:550\u001b[0m, in \u001b[0;36mDataIter._handle_exception\u001b[0;34m(self, fn, dft_ret)\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dft_ret\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;66;03m# Defer the exception in order to return 0 and stop the iteration.\u001b[39;00m\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;66;03m# Exception inside a ctype callback function has no effect except\u001b[39;00m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;66;03m# for printing to stderr (doesn't stop the execution).\u001b[39;00m\n\u001b[1;32m    555\u001b[0m     tb \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/core.py:637\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_exception(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/data.py:1416\u001b[0m, in \u001b[0;36mSingleBatchInternalIter.next\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m   1414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mit \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1416\u001b[0m \u001b[43minput_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/core.py:617\u001b[0m, in \u001b[0;36mDataIter._next_wrapper.<locals>.input_data\u001b[0;34m(data, feature_names, feature_types, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m     new, cat_codes, feature_names, feature_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 617\u001b[0m     new, cat_codes, feature_names, feature_types \u001b[38;5;241m=\u001b[39m \u001b[43m_proxy_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_enable_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;66;03m# Stage the data, meta info are copied inside C++ MetaInfo.\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temporary_data \u001b[38;5;241m=\u001b[39m (new, cat_codes, feature_names, feature_types)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/data.py:1459\u001b[0m, in \u001b[0;36m_proxy_transform\u001b[0;34m(data, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m   1457\u001b[0m     data \u001b[38;5;241m=\u001b[39m _arrow_transform(data)\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_pandas_df(data):\n\u001b[0;32m-> 1459\u001b[0m     df, feature_names, feature_types \u001b[38;5;241m=\u001b[39m \u001b[43m_transform_pandas_df\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_types\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1462\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df, \u001b[38;5;28;01mNone\u001b[39;00m, feature_names, feature_types\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue type is not supported for data iterator:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(data)))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/data.py:603\u001b[0m, in \u001b[0;36m_transform_pandas_df\u001b[0;34m(data, enable_categorical, feature_names, feature_types, meta)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_transform_pandas_df\u001b[39m(\n\u001b[1;32m    597\u001b[0m     data: DataFrame,\n\u001b[1;32m    598\u001b[0m     enable_categorical: \u001b[38;5;28mbool\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    601\u001b[0m     meta: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    602\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[PandasTransformed, Optional[FeatureNames], Optional[FeatureTypes]]:\n\u001b[0;32m--> 603\u001b[0m     \u001b[43mpandas_check_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menable_categorical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m meta \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _matrix_meta:\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmeta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot have multiple columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/data.py:569\u001b[0m, in \u001b[0;36mpandas_check_dtypes\u001b[0;34m(data, enable_categorical)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dtype \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtypes:\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m    564\u001b[0m         (dtype\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m _pandas_dtype_mapper)\n\u001b[1;32m    565\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m is_pd_sparse_dtype(dtype)\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (is_pd_cat_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m enable_categorical)\n\u001b[1;32m    567\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m is_pa_ext_dtype(dtype)\n\u001b[1;32m    568\u001b[0m     ):\n\u001b[0;32m--> 569\u001b[0m         \u001b[43m_invalid_dataframe_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_pd_sparse_dtype(dtype):\n\u001b[1;32m    572\u001b[0m         sparse_extension \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/xgboost/data.py:356\u001b[0m, in \u001b[0;36m_invalid_dataframe_dtype\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    354\u001b[0m type_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame.dtypes for data must be int, float, bool or category.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    355\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_err\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_ENABLE_CAT_ERR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 356\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, the experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:Equipment_Dam: object, Model.Suffix_Dam: object, Workorder_Dam: object, Chamber Temp. Judge Value_AutoClave: object, Equipment_Fill1: object, Equipment_Fill2: object"
     ]
    }
   ],
   "source": [
    "# 모델 학습 (예: XGBoost)\n",
    "model = xgb.XGBClassifier(random_state=RANDOM_STATE,enable_categorical=True)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plAgAQyojNYz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "plAgAQyojNYz",
    "outputId": "487da53f-9630-48d3-cd7d-fdcd9d0079ed"
   },
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac085d9",
   "metadata": {
    "id": "0ac085d9"
   },
   "outputs": [],
   "source": [
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5e80f8",
   "metadata": {
    "id": "8a5e80f8"
   },
   "source": [
    "중요도는 피처가 모델 예측에 미치는 영향력을 의미하며, 값의 크기와는 무관하다.\n",
    "SHAP 값은 피처의 중요도를 측정하는 도구로, 피처 값의 크기가 아닌, 그 피처가 예측에 얼마나 기여했는지를 기반으로 중요도를 평가함.\n",
    "\n",
    "- 따라서 모델의 성능을 최적화하기 위해서는 피처의 절대적인 값의 크기보다는, 피처의 예측 기여도(중요도)를 평가하는 것이 중요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FR91D9NykFUn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "FR91D9NykFUn",
    "outputId": "d531da81-360b-4c8c-a35b-786aaf1b68c8"
   },
   "outputs": [],
   "source": [
    "# SHAP 값 계산을 위한 explainer 생성\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_val)\n",
    "\n",
    "# SHAP 값 시각화 (각 피처의 중요도)\n",
    "shap.summary_plot(shap_values, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf93d4d",
   "metadata": {
    "id": "eaf93d4d"
   },
   "outputs": [],
   "source": [
    "# 5. SHAP 중요도를 기반으로 피처 선택\n",
    "# SHAP 값의 절대값 평균을 구하여 중요도를 평가ㅏ\n",
    "shap_importance = pd.DataFrame({\n",
    "    'feature': X_val.columns,\n",
    "    'importance': np.abs(shap_values).mean(axis=0)\n",
    "}).sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57fb1cc",
   "metadata": {
    "id": "d57fb1cc"
   },
   "outputs": [],
   "source": [
    "# 중요 피처 기준 설정 (예: 상위 20개 피처만 선택)\n",
    "threshold = 70\n",
    "important_features = shap_importance['feature'].head(threshold).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de45c3d0",
   "metadata": {
    "id": "de45c3d0"
   },
   "outputs": [],
   "source": [
    "# 중요 피처만을 선택하여 새로운 데이터셋 생성\n",
    "X_train_selected = X_train[important_features]\n",
    "X_val_selected = X_val[important_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9427d0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "359719f0",
   "metadata": {
    "id": "359719f0"
   },
   "source": [
    "## 모델 학습 및 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830585ca",
   "metadata": {
    "id": "830585ca"
   },
   "source": [
    "여기서 하이퍼 파라미터는 GPU 돌려서 가장 잘 나오는 하이퍼파리미터로 선정했다.\n",
    "\n",
    "0823_Leem_GPU 참조\n",
    "* 이후 모델 성능 높인후 마지막에 한번 더 최적의 성능을 만드는 하이퍼파라미터 골라야함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bafc117",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "5bafc117",
    "outputId": "d0160dac-2606-43d2-9e2e-6814e2d242a0"
   },
   "outputs": [],
   "source": [
    "# 선택된 피처로 모델 재학습\n",
    "model_selected = xgb.XGBClassifier(\n",
    "    colsample_bytree=0.8,\n",
    "    gamma=0.1,\n",
    "    learning_rate=0.01,\n",
    "    max_depth=7,\n",
    "    min_child_weight=1,\n",
    "    n_estimators=200,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=1,\n",
    "    subsample=0.8,\n",
    "    eval_metric='logloss',\n",
    "    random_state=RANDOM_STATE,\n",
    "    enable_categorical=True\n",
    ")\n",
    "model_selected.fit(X_train_selected, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341cfa3d",
   "metadata": {
    "id": "341cfa3d"
   },
   "source": [
    "### 검증데이터로 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8FHY27zJhs85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8FHY27zJhs85",
    "outputId": "b7a54087-d36d-4106-a1bd-12bf45ecae24"
   },
   "outputs": [],
   "source": [
    "# ordianl 인코딩\n",
    "y_pred_original = model.predict(X_val)\n",
    "accuracy_original = accuracy_score(y_val, y_pred_original)\n",
    "report_original = classification_report(y_val, y_pred_original)\n",
    "f1_score_class_original = f1_score(y_val, y_pred_original, average='binary', pos_label=0)\n",
    "print(f\"Validation Accuracy (Original): {accuracy_original:.4f}\")\n",
    "print(f\"F1 Score for Class (Original):                    {f1_score_class_original:.4f}\")\n",
    "print(\"Classification Report (Original):\")\n",
    "print(report_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907fbe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target 인코딩\n",
    "y_pred_original = model.predict(X_val)\n",
    "accuracy_original = accuracy_score(y_val, y_pred_original)\n",
    "report_original = classification_report(y_val, y_pred_original)\n",
    "f1_score_class_original = f1_score(y_val, y_pred_original, average='binary', pos_label=0)\n",
    "print(f\"Validation Accuracy (Original): {accuracy_original:.4f}\")\n",
    "print(f\"F1 Score for Class (Original):                    {f1_score_class_original:.4f}\")\n",
    "print(\"Classification Report (Original):\")\n",
    "print(report_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BiXBkh0a96d1",
   "metadata": {
    "id": "BiXBkh0a96d1"
   },
   "source": [
    "# 여기서 부터는 Feature 선택이후"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pXF0kQ7pf7UW",
   "metadata": {
    "id": "pXF0kQ7pf7UW"
   },
   "outputs": [],
   "source": [
    "# 피처 선택 후 # ordianl 인코딩\n",
    "y_pred_selected = model_selected.predict(X_val_selected)\n",
    "accuracy_selected = accuracy_score(y_val, y_pred_selected)\n",
    "f1_score_class_0 = f1_score(y_val, y_pred_selected, average='binary', pos_label=0)\n",
    "report_selected = classification_report(y_val, y_pred_selected)\n",
    "print(f\"Validation Accuracy (Selected Features): {accuracy_selected:.4f}\")\n",
    "print(f\"F1 Score for Class 0:                    {f1_score_class_0:.4f}\")\n",
    "print(\"Classification Report (Selected Features):\")\n",
    "print(report_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86189f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피처 선택 후 target 인코딩\n",
    "y_pred_selected = model_selected.predict(X_val_selected)\n",
    "accuracy_selected = accuracy_score(y_val, y_pred_selected)\n",
    "f1_score_class_0 = f1_score(y_val, y_pred_selected, average='binary', pos_label=0)\n",
    "report_selected = classification_report(y_val, y_pred_selected)\n",
    "print(f\"Validation Accuracy (Selected Features): {accuracy_selected:.4f}\")\n",
    "print(f\"F1 Score for Class 0:                    {f1_score_class_0:.4f}\")\n",
    "print(\"Classification Report (Selected Features):\")\n",
    "print(report_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328ff03c",
   "metadata": {
    "id": "328ff03c"
   },
   "source": [
    "### 5. 최종 예측 및 제출파일 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb889f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "fdb889f4",
    "outputId": "c299c14f-3d05-4c55-b3bf-eb63161c23f0"
   },
   "outputs": [],
   "source": [
    "# 4. submission.csv 파일 불러오기\n",
    "submission = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RE5ntoewQsDT",
   "metadata": {
    "id": "RE5ntoewQsDT"
   },
   "outputs": [],
   "source": [
    "# 10. 최종 모델 선택 후 테스트 데이터 예측\n",
    "# 중요 피처를 이용해 테스트 데이터셋을 필터링\n",
    "X_test_selected = test_data_encoded[important_features]\n",
    "\n",
    "# 모델을 사용해 예측 수행\n",
    "y_test_pred = model_selected.predict(X_test_selected)\n",
    "\n",
    "# 11. 예측 결과를 'target' 열로 추가\n",
    "target_map = {1: 'Normal', 0: 'AbNormal'}\n",
    "y_test_pred_mapped = pd.Series(y_test_pred).map(target_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696fe020",
   "metadata": {
    "id": "696fe020"
   },
   "outputs": [],
   "source": [
    "# 5. 예측 결과를 'target' 열에 넣기\n",
    "submission['target'] = y_test_pred_mapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5823e",
   "metadata": {
    "id": "efc5823e"
   },
   "outputs": [],
   "source": [
    "# 6. 결과를 확인하고 저장하기 (옵션)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "558194e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'submission' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[140], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msubmission\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'submission' is not defined"
     ]
    }
   ],
   "source": [
    "submission['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2p0kAVm8aG5",
   "metadata": {
    "id": "d2p0kAVm8aG5"
   },
   "source": [
    "###Ordinal Encoding은 데이터에 순서가 있는 경우 간단하고 해석 가능한 방법으로 유용합니다. 그러나 순서 간의 간격이 균등하다고 가정하기 때문에 모델이 이를 정확히 반영하지 못할 수 있습니다.\n",
    "\n",
    "###Target Encoding은 범주형 변수와 목표 변수 간의 관계를 반영하므로 성능이 향상될 수 있습니다. 그러나 과적합과 데이터 누수의 위험이 있으며, 인코딩 과정이 복잡할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c37b788",
   "metadata": {},
   "source": [
    "# ordinal 인코딩 0.17583030979626013"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a73ec98",
   "metadata": {
    "id": "dXcyTyz98bAK"
   },
   "source": [
    "# target 인코딩 0.17344934044344654"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899c18f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
